{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9l0lyqLvkW7"
   },
   "source": [
    "# Tutorial 1\n",
    "\n",
    "**Machine Learning III: Clustering & Classification**\n",
    "\n",
    "**[insert your name]**\n",
    "\n",
    "**Important reminders**: Before starting, click \"[rocket item in top right] -> Colab\" to open the notebook in Colab, and then choose \"File -> Save a copy in Drive\" to save a local copy. Produce a pdf for submission by \"File -> Print\" and then choose \"Save to PDF\".\n",
    "\n",
    "**We use the dataset and some of the text/code from NMA W1D4 T2.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cv9HSBNPyLV9"
   },
   "outputs": [],
   "source": [
    "# @markdown Imports\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets  # interactive display\n",
    "import math\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZIdPVYl9TzmK"
   },
   "outputs": [],
   "source": [
    "# @markdown Plotting functions\n",
    "import numpy\n",
    "from numpy.linalg import inv, eig\n",
    "from math import ceil\n",
    "from matplotlib import pyplot, ticker, get_backend, rc\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "\n",
    "\n",
    "def plot_function(f, name, var, points=(-10, 10)):\n",
    "    \"\"\"Evaluate f() on linear space between points and plot.\n",
    "\n",
    "    Args:\n",
    "      f (callable): function that maps scalar -> scalar\n",
    "      name (string): Function name for axis labels\n",
    "      var (string): Variable name for axis labels.\n",
    "      points (tuple): Args for np.linspace to create eval grid.\n",
    "    \"\"\"\n",
    "    x = np.linspace(*points)\n",
    "    ax = plt.figure().subplots()\n",
    "    ax.plot(x, f(x))\n",
    "    ax.set(\n",
    "      xlabel=f'${var}$',\n",
    "      ylabel=f'${name}({var})$'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0TCUlgD2L2y7"
   },
   "outputs": [],
   "source": [
    "# @markdown Helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7KQoWsM6hZE"
   },
   "source": [
    "# The data\n",
    "\n",
    "We will use a dataset that includes recordings of neurons as mice perform a decision task.\n",
    "\n",
    "Mice had the task of turning a wheel to indicate whether they perceived a Gabor stimulus to the left, to the right, or not at all. Neuropixel probes measured spikes across the cortex.\n",
    "Today we're going to **decode the decision from neural data** using Logistic Regression. We will only consider trials where the mouse chose \"Left\" or \"Right\" and ignore NoGo trials.\n",
    "\n",
    "### Data format\n",
    "\n",
    "In the hidden `Data retrieval and loading` cell, there is a function that loads the data:\n",
    "\n",
    "- `spikes`: an array of normalized spike rates with shape `(n_trials, n_neurons)`\n",
    "- `choices`: a vector of 0s and 1s, indicating the animal's behavioral response, with length `n_trials`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mMLBDwD86gnk"
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval and loading\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "url = \"https://osf.io/r9gh8/download\"\n",
    "fname = \"W1D4_steinmetz_data.npz\"\n",
    "expected_md5 = \"d19716354fed0981267456b80db07ea8\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "      print(\"!!! Data download appears corrupted !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "\n",
    "def load_steinmetz_data(data_fname=fname):\n",
    "\n",
    "  with np.load(data_fname) as dobj:\n",
    "    data = dict(**dobj)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hh43k-fH61SM",
    "outputId": "b115ea90-45e0-4615-e7b9-079b5f21348f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikes (276, 691)\n",
      "choices (276,)\n"
     ]
    }
   ],
   "source": [
    "data = load_steinmetz_data()\n",
    "for key, val in data.items():\n",
    "  print(key, val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DkRkmFO7I1X"
   },
   "source": [
    "We will rename our variables to make clear which is the input and which is the output variable. We will also split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PVKC7xC7QQX",
    "outputId": "b240c0b3-0cac-4275-b4a1-611e862b721f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 691)\n",
      "(220,)\n"
     ]
    }
   ],
   "source": [
    "n_trials = data[\"choices\"].shape[0]\n",
    "inds = np.arange(n_trials)\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "train_inds = inds[:int(.8*n_trials)]\n",
    "test_inds = inds[int(.8*n_trials):]\n",
    "\n",
    "y = data[\"choices\"][train_inds]\n",
    "X = data[\"spikes\"][train_inds]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaQWiL646nLP"
   },
   "source": [
    "# Exercise 1: Coding & applying logistic regression\n",
    "\n",
    "We will write our own logistic regression code, instead of using sklearn, to make sure we really understand what's going on with this model.\n",
    "\n",
    "**If you want challenge mode, don't follow steps A-D below, just try doing it yourself without the guided steps! Join for step E and F. We'll accept that for submission as long as you write clearly what you've done**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgUdwHxM75TM"
   },
   "source": [
    "## A) Compute z\n",
    "\n",
    "For our first step, want to code a function that takes in the input and outputs z (the value that will go into the sigmoid):\n",
    "\n",
    "$$ z = X\\theta$$\n",
    "\n",
    "First figure out what size $\\theta$ should be and create a fake $\\theta$ full of zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sz_1tbxMDtr-"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1tH9A_X737R"
   },
   "outputs": [],
   "source": [
    "def transform_inputs(theta, X):\n",
    "\n",
    "  return ...\n",
    "\n",
    "theta = np.zeros(...)\n",
    "\n",
    "z = transform_inputs(theta, X)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCfdexBI7eS4"
   },
   "source": [
    "## B) Sigmoid\n",
    "\n",
    "Now, we need to code our sigmoid function:\n",
    "\n",
    "$$ h = \\frac{1}{1 + \\textrm{exp}(-z)}$$\n",
    "\n",
    "Please complete the function below and make sure the resulting plot looks correct to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZR2CeurC3NX"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBC-BxGI6uk7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  \"\"\"Return the logistic transform of z.\"\"\"\n",
    "\n",
    "  return ...\n",
    "\n",
    "plot_function(sigmoid, \"\\sigma\", \"z\", (-10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tYlpL-b8n8-"
   },
   "source": [
    "## C) Compute h\n",
    "\n",
    "We want to compute h now as:\n",
    "\n",
    "$$ z = X\\theta$$\n",
    "$$h = \\frac{1}{1 + \\textrm{exp}(-z)}$$\n",
    "\n",
    "Complete the code below to do this (call the functions you've created in parts A and B)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuxD6v54C7Ra"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNBgzxRw8MND"
   },
   "outputs": [],
   "source": [
    "def compute_h(theta, X):\n",
    "\n",
    "    # Returns h after passing through sigmoid\n",
    "    return ...\n",
    "\n",
    "h = compute_h(theta, X)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDul8SPO9EEF"
   },
   "source": [
    "## D) Compute the cost function\n",
    "\n",
    "Now we want to be able to compute the cost function with theta, X, and y as our inputs.\n",
    "\n",
    "Remember that:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum_i y_i log(h_i) + (1 - y_i) log(1-h_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZW-Us9EDBsF"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmB9kZP589Ga"
   },
   "outputs": [],
   "source": [
    "def logistic_loss(theta, X, y):\n",
    "    # Computes the cost function for all the training samples\n",
    "    ...\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "total_cost = logistic_loss(theta, X, y)\n",
    "print(total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBTMY59D98EM"
   },
   "source": [
    "## E) Seeing the results\n",
    "\n",
    "Now we can use `scipy.optimize.minimize` to perform gradient descent and see our results! This will take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6_7ik8L9sR-"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "outs = minimize(logistic_loss, theta, (X, y))\n",
    "\n",
    "theta_hat = outs.x\n",
    "\n",
    "print(theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZeYZry4_nX7"
   },
   "source": [
    "Use $\\hat{\\theta}$ to determine the accuracy on of the classification. We won't use a function for this. Remember that you need to get h, then transform this to $\\hat{y}$ (where $\\hat{y}_i$ is 1 if h is greater than or equal to 0.5, otherwise it is 0), then compare $\\hat{y}$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcBGTZ0qDLSW"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-q7Wqsa-Y2z"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(theta_hat, X, y):\n",
    "  ...\n",
    "  return ...\n",
    "\n",
    "acc = evaluate_accuracy(theta_hat, X, y)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sjf32q-QdkG"
   },
   "source": [
    "# Exercise 2: Assessing accuracy\n",
    "\n",
    "How do we know if this accuracy is above chance? Let's shuffle the trial decisions 1000 times, compute accuracies of logistic regression fit on the shuffled data, and compare.  \n",
    "\n",
    "We will actually use a sklearn logistic regression model to fit our models as it's a little faster. See the cell below for how to use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIVN5-1_STbo"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "acc = model.score(X, y)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKrXW0OCSZJe"
   },
   "source": [
    "## A) Shuffle test\n",
    "\n",
    "Now complete the code below to shuffle the decisions taken on each trial. Fit a model on that shuffled data and record the accuracy in `shuffled_accuracies`. We plot a histogram of the accuracies for all these shuffled samples and compare it to the true accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2PO67v9Qi_6"
   },
   "outputs": [],
   "source": [
    "n_shuffles = 1000\n",
    "\n",
    "shuffled_accuracies = np.zeros((n_shuffles,))\n",
    "\n",
    "for i_shuffle in range(n_shuffles):\n",
    "\n",
    "    ... # your code here to shuffle the labels\n",
    "\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    model.fit(...)\n",
    "\n",
    "    shuffled_accuracies[i_shuffle] = model.score(...)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "ax.hist(shuffled_accuracies, 100);\n",
    "ax.plot([acc, acc], ax.get_ylim(), 'r', linewidth = 2, label = 'Unshuffled accuracy')\n",
    "ax.legend()\n",
    "ax.set(xlabel = 'Accuracy', ylabel = 'Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k5Qt9etTckn"
   },
   "source": [
    "## B) Interpreting results\n",
    "\n",
    "Are the accuracies for the shuffled data samples higher than you expected? Do you think you can conclude from the true accuracy that the neural responses have a significant amount of information about the decision? Why or why not?\n",
    "\n",
    "Any initial thoughts about why we couldn't just check whether our true accuracy is above 50%, which we may naively assume is chance performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wh_t3GFecxum"
   },
   "source": [
    "Your answers here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNQk1JWddBJ2SN3G5CgAg+Q",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
